import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc, RocCurveDisplay
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import seaborn as sns
sns.set(style="whitegrid")



# Load data
df = pd.read_csv("../../holly_extended.csv")
df





df['sex'] = df['sex'].str.strip().str.lower()
df['diagnosis'] = df['clinicalStatus'].str.strip().str.lower()

# Define robust mappings
sex_map = {'m': 0, 'male': 0, 'f': 1, 'female': 1}
diagnosis_map = {'hc': 0, 'healthy': 0, 'mci': 1}

# Map strings to integers with .map()
df['sex'] = df['sex'].map(sex_map)
df['diagnosis'] = df['diagnosis'].map(diagnosis_map)

# Feature selection and scaling
X = df[['age', 'sex', 'MoCA', 'SoF']].values
y = df['diagnosis'].values



scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Build the model
model = Sequential([
    Dense(16, activation='relu', input_shape=(4,)),
    Dense(8, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=0)

# Evaluate with ROC
y_scores = model.predict(X_test).ravel()
#fpr, tpr, _ = roc_curve(y_test, y_scores)
#roc_auc = auc(fpr, tpr)


# SHAP explainability
explainer = shap.Explainer(model, X_train[:20])
shap_values = explainer(X_test[:10])

shap.summary_plot(shap_values, X_test[:10], feature_names=['age', 'sex', 'moca_score', 'memory_score'])

fig, ax = plt.subplots(1)
RocCurveDisplay.from_predictions(y_test, y_scores, 
                                 ax=ax, 
                                 pos_label=None, plot_chance_level=True)
plt.show()





import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv("../../holly_extended.csv")

df['sex'] = df['sex'].str.strip().str.lower()

# Define robust mappings
sex_map = {'m': 0, 'male': 0, 'f': 1, 'female': 1}

# Map strings to integers with .map()
df['sex'] = df['sex'].map(sex_map)

# Separate input features
features = ['age', 'sex', 'MoCA', 'Craft_21_Delayed_Score',	'BCFC_Immeadiate']
X = df[features].copy()

# 1. Pre-impute with mean (to allow training)
mean_imputer = SimpleImputer(strategy='mean')
X_imputed = mean_imputer.fit_transform(X)

# 2. Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# 3. Build autoencoder
input_dim = X_scaled.shape[1]
encoding_dim = 3  # compression

input_layer = Input(shape=(input_dim,))
encoded = Dense(encoding_dim, activation='relu')(input_layer)
decoded = Dense(input_dim, activation='linear')(encoded)

autoencoder = Model(input_layer, decoded)
autoencoder.compile(optimizer=Adam(0.001), loss='mse')

# 4. Train autoencoder
autoencoder.fit(X_scaled, X_scaled, epochs=100, batch_size=8, verbose=0)

# 5. Impute missing data using the trained autoencoder
# Mask original rows with missing values
mask_missing = X.isnull().any(axis=1)

# Scale the mean-imputed data again
X_imputed_scaled = scaler.transform(mean_imputer.transform(X))

# Predict (reconstruct) from the autoencoder
X_reconstructed = autoencoder.predict(X_imputed_scaled)

# Inverse transform to return to original scale
X_reconstructed_unscaled = scaler.inverse_transform(X_reconstructed)

# Replace missing entries in original df with reconstructed values
X_filled = X.copy()
for i in range(X.shape[0]):
    for j in range(X.shape[1]):
        if pd.isnull(X.iloc[i, j]):
            X_filled.iat[i, j] = X_reconstructed_unscaled[i, j]

# Final filled DataFrame
df_imputed = df.copy()
df_imputed[features] = X_filled

# Save or display
print(df_imputed.head())






import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load your data (simulate here for demo)
# Normally: X = np.load('brain_images.npy')  # Shape: (N, 80, 80)
#           y = np.load('labels.npy')        # Shape: (N,)

# Simulate data
N = 100  # number of patients
X = np.random.rand(N, 80, 80, 1)  # 1 channel (grayscale)
y = np.random.randint(0, 2, size=N)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define CNN model
model = Sequential([
    Conv2D(16, (3, 3), activation='relu', input_shape=(80, 80, 1)),
    MaxPooling2D((2, 2)),
    
    Conv2D(32, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Compile model
model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train, epochs=15, batch_size=8, validation_split=0.1, verbose=1)

# Evaluate on test set
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {test_acc:.2f}")

# Plot training curves
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Val')
plt.title('CNN Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

